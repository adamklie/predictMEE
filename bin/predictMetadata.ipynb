{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Entities<br>Adam Klie<br>11/02/2019<br>Script to predict entities in from trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Data visualization\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from string import punctuation\n",
    "\n",
    "# Neural nets\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed tokens from text into word embedding space\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = np.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import trained model, word embedding, and data to build validation data of off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iter = 'simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('../data/wikipedia-pubmed-and-PMC-w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load('../results/{model}_classes.npy'.format(model = model_iter))\n",
    "model = load_model('../models/{model}.h5'.format(model = model_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.predict_proba(get_features(nlp.pipe([\"BRAF mutation\"]), 7)), columns=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRS_dir = \"../data/allSRS_05_15_2018.pickle\"\n",
    "allSRS = pd.read_pickle(SRS_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random titles as validation data\n",
    "#random_data = allSRS[allSRS.index.get_level_values(1) == 'TITLE'].sample(n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in validation data for a specific class to predict on\n",
    "validation_class = 'Tissue'\n",
    "filename = '../results/{myclass}_validation_set.pickle'.format(model = model_iter, myclass = validation_class)\n",
    "readin_data = pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_data = readin_data\n",
    "validation_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking up into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test = validation_data.str.split('[;.,]', expand = True).stack()\n",
    "processed_test = processed_test.str.replace('\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the empty state to use as baseline probability emission\n",
    "val_docs = list(nlp.pipe(' '))\n",
    "val_X = get_features(val_docs, max_length = model.input_shape[1])\n",
    "emptyState = model.predict_proba(val_X)[0,:]\n",
    "emptyState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stopWords = set(stopwords.words('english'))\n",
    "rows = []\n",
    "key_list = []\n",
    "for i, (key, sent) in enumerate(tqdm(processed_test.items(), total=len(processed_test))):\n",
    "    \n",
    "    # Sentence preprocessing\n",
    "    #sent = re.sub(r'[^a-zA-Z0-9]+', ' ', sent)  # remove non alpha numeric characters\n",
    "    tokens = re.split(pattern = ' ', string = sent)  # tokenize the description\n",
    "    tokens = list(filter(lambda token:(token!='') and (token not in stopWords), tokens))  # filter out stopwords\n",
    "    sent = ' '.join(tokens)\n",
    "    \n",
    "    n_gram_max = min([len(tokens), 7])\n",
    "    for n_gram in range(2, n_gram_max + 1):\n",
    "        \n",
    "        # Get prediction for all current n-grams\n",
    "        grams = list(map(lambda L:\" \".join(L), list(ngrams(tokens, n_gram))))  \n",
    "        val_docs = list(nlp.pipe(grams))  # get spacy objects for each token passed in\n",
    "        val_X = get_features(val_docs, max_length = model.input_shape[1])\n",
    "        predictM = model.predict_proba(val_X)\n",
    "        \n",
    "        # Take only those n-grams that have a total probability greater than the empty state + 0.01\n",
    "        # and also have two tokens present in word-embedding\n",
    "        tmp_df = pd.DataFrame(data = predictM, columns = le.classes_, index = grams)\n",
    "        empty_mask = (tmp_df - emptyState).abs().sum(axis=1) < 0.01\n",
    "        moreThanTwoValToken_mask = (val_X != 0).sum(axis=1) >= 2\n",
    "        tmp_df[empty_mask&moreThanTwoValToken_mask] = 0\n",
    "        \n",
    "        # Set up keys for dataframe with probabilities of each n-gram, will be useful later\n",
    "        for j, gram in enumerate(tmp_df.index):\n",
    "            i_end = j + n_gram\n",
    "            textBefore = \" \".join(tokens[:j]) + ('' if j==0 else ' ')\n",
    "            start_char_pos = len(textBefore)\n",
    "            key_list.append(key + (i, sent, n_gram, j, i_end, gram, start_char_pos)) \n",
    "            rows.append(tmp_df.iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_df = pd.concat(rows, keys = key_list, axis = 1).T\n",
    "proba_df.index.names = ['srs', 'attribute', 'sentence_number', 'kthSrs', \n",
    "                        'orig_text', 'n-gram_length', 'word_start', 'word_end', 'token', \n",
    "                        'starting_char_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textS = pd.Series(proba_df.index.get_level_values('orig_text').unique())\n",
    "textM = textS.str.count(' ') >= 0\n",
    "selectedTexts = textS[textM].values # get the original texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threshold = 2\n",
    "proba_sub = proba_df[(proba_df.index.get_level_values('n-gram_length') >= n_threshold) &\n",
    "                     (proba_df.index.get_level_values('orig_text').isin(selectedTexts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_proba = proba_sub.max(axis=1)\n",
    "second_proba = proba_sub.quantile(0.999, interpolation='lower', axis = 1)\n",
    "scoreMargin_m = (max_proba-second_proba) > 0.1  # proba difference between 1st and 2nd must be greater than 0.1\n",
    "m_val = scoreMargin_m & (~proba_sub.index.get_level_values('token').str.contains('[0-9 ]+ [0-9 ]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf = pd.DataFrame({'predicted':proba_sub[m_val].idxmax(axis=1),'score':proba_sub[m_val].max(axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreSortedDf = tmpDf[m_val].sort_values(['orig_text','word_start','score'], ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = scoreSortedDf.copy()\n",
    "scoreSortedDf = scoreSortedDf.assign(OverlapGroup=(len(processed_test)*(v.kthSrs)+ \n",
    "                                          (v.word_end - v.word_start.shift(-1)).shift().lt(0).cumsum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf=scoreSortedDf.sort_values(['OverlapGroup','score'],ascending=False).drop_duplicates(['OverlapGroup','predicted']\n",
    "                                                                                   ).sort_values('orig_text')\n",
    "hitDf['token_len']=hitDf['token'].str.len()\n",
    "hitDf['recovered_txt']=hitDf.apply(\n",
    "    lambda tmpS2:tmpS2.loc['orig_text'][tmpS2.loc['starting_char_pos']:(tmpS2.loc['starting_char_pos']+tmpS2.loc['token_len'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf.to_pickle(\n",
    "    '../results/{myclass}_{model}_prediction.pickle'.format(model = model_iter, myclass = validation_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
